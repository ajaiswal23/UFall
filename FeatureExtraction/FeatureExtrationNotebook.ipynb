{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#This is an example of performing Video Activity Recognition using LSTM\n",
    "Modified from \"Hands-on Computer Vision with TensorFlow 2\" by B. Planche and E. Andres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 1:  Install packages in the current environment\n",
    "import sys\n",
    "!{sys.executable} -m pip install opencv-python\n",
    "!{sys.executable} -m pip install matplotlib\n",
    "!{sys.executable} -m pip install tqdm\n",
    "!{sys.executable} -m pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow==2.0 in c:\\users\\stsc.lnvo-126908.000\\appdata\\roaming\\python\\python37\\site-packages (2.0.0)\n",
      "Requirement already satisfied: absl-py>=0.7.0 in c:\\users\\stsc.lnvo-126908.000\\anaconda3\\lib\\site-packages (from tensorflow==2.0) (0.8.1)\n",
      "Requirement already satisfied: gast==0.2.2 in c:\\users\\stsc.lnvo-126908.000\\anaconda3\\lib\\site-packages (from tensorflow==2.0) (0.2.2)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\stsc.lnvo-126908.000\\anaconda3\\lib\\site-packages (from tensorflow==2.0) (3.1.0)\n",
      "Requirement already satisfied: protobuf>=3.6.1 in c:\\users\\stsc.lnvo-126908.000\\anaconda3\\lib\\site-packages (from tensorflow==2.0) (3.11.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.6 in c:\\users\\stsc.lnvo-126908.000\\anaconda3\\lib\\site-packages (from tensorflow==2.0) (0.1.8)\n",
      "Requirement already satisfied: keras-preprocessing>=1.0.5 in c:\\users\\stsc.lnvo-126908.000\\anaconda3\\lib\\site-packages (from tensorflow==2.0) (1.1.0)\n",
      "Requirement already satisfied: astor>=0.6.0 in c:\\users\\stsc.lnvo-126908.000\\anaconda3\\lib\\site-packages (from tensorflow==2.0) (0.8.0)\n",
      "Requirement already satisfied: grpcio>=1.8.6 in c:\\users\\stsc.lnvo-126908.000\\anaconda3\\lib\\site-packages (from tensorflow==2.0) (1.25.0)\n",
      "Requirement already satisfied: tensorboard<2.1.0,>=2.0.0 in c:\\users\\stsc.lnvo-126908.000\\anaconda3\\lib\\site-packages (from tensorflow==2.0) (2.0.2)\n",
      "Requirement already satisfied: six>=1.10.0 in c:\\users\\stsc.lnvo-126908.000\\anaconda3\\lib\\site-packages (from tensorflow==2.0) (1.12.0)\n",
      "Requirement already satisfied: tensorflow-estimator<2.1.0,>=2.0.0 in c:\\users\\stsc.lnvo-126908.000\\anaconda3\\lib\\site-packages (from tensorflow==2.0) (2.0.1)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\stsc.lnvo-126908.000\\anaconda3\\lib\\site-packages (from tensorflow==2.0) (1.1.0)\n",
      "Requirement already satisfied: wrapt>=1.11.1 in c:\\users\\stsc.lnvo-126908.000\\anaconda3\\lib\\site-packages (from tensorflow==2.0) (1.11.2)\n",
      "Requirement already satisfied: numpy<2.0,>=1.16.0 in c:\\users\\stsc.lnvo-126908.000\\appdata\\roaming\\python\\python37\\site-packages (from tensorflow==2.0) (1.17.4)\n",
      "Requirement already satisfied: wheel>=0.26 in c:\\users\\stsc.lnvo-126908.000\\anaconda3\\lib\\site-packages (from tensorflow==2.0) (0.33.6)\n",
      "Requirement already satisfied: keras-applications>=1.0.8 in c:\\users\\stsc.lnvo-126908.000\\anaconda3\\lib\\site-packages (from tensorflow==2.0) (1.0.8)\n",
      "Requirement already satisfied: setuptools in c:\\users\\stsc.lnvo-126908.000\\anaconda3\\lib\\site-packages (from protobuf>=3.6.1->tensorflow==2.0) (41.4.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\stsc.lnvo-126908.000\\anaconda3\\lib\\site-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow==2.0) (3.1.1)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in c:\\users\\stsc.lnvo-126908.000\\anaconda3\\lib\\site-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow==2.0) (0.16.0)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\stsc.lnvo-126908.000\\anaconda3\\lib\\site-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow==2.0) (2.22.0)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in c:\\users\\stsc.lnvo-126908.000\\anaconda3\\lib\\site-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow==2.0) (1.7.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in c:\\users\\stsc.lnvo-126908.000\\anaconda3\\lib\\site-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow==2.0) (0.4.1)\n",
      "Requirement already satisfied: h5py in c:\\users\\stsc.lnvo-126908.000\\anaconda3\\lib\\site-packages (from keras-applications>=1.0.8->tensorflow==2.0) (2.9.0)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\stsc.lnvo-126908.000\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0) (1.24.2)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in c:\\users\\stsc.lnvo-126908.000\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\stsc.lnvo-126908.000\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0) (2019.9.11)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in c:\\users\\stsc.lnvo-126908.000\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0) (2.8)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\stsc.lnvo-126908.000\\anaconda3\\lib\\site-packages (from google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0) (0.2.7)\n",
      "Requirement already satisfied: cachetools<3.2,>=2.0.0 in c:\\users\\stsc.lnvo-126908.000\\anaconda3\\lib\\site-packages (from google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0) (3.1.1)\n",
      "Requirement already satisfied: rsa<4.1,>=3.1.4 in c:\\users\\stsc.lnvo-126908.000\\anaconda3\\lib\\site-packages (from google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0) (4.0)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\stsc.lnvo-126908.000\\anaconda3\\lib\\site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0) (1.3.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\users\\stsc.lnvo-126908.000\\anaconda3\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\stsc.lnvo-126908.000\\anaconda3\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0) (3.1.0)\n"
     ]
    }
   ],
   "source": [
    "#!{sys.executable} -m pip install tensorflow==1.14\n",
    "!{sys.executable} -m pip install tensorflow==2.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Step 2: import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tqdm\n",
    "from sklearn.preprocessing import LabelBinarizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tf.enable_eager_execution()\n",
    "#tf.executing_eagerly()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Step 3: setup variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#location of where YOU have installed the data set UCF-101 located\n",
    "# at \n",
    "#BASE_PATH = '../data/UCF-101'\n",
    "#change the base path to location YOU installed UCF-101 dataset\n",
    "#BASE_PATH = 'C:/Grewe/Classes/CS663/Mat/LSTM/data/UCF-101'\n",
    "BASE_PATH = 'C:\\\\Users\\\\STSC.LNVO-126908.000\\\\Desktop\\\\FallDetection\\\\Classes\\\\'\n",
    "VIDEOS_PATH = os.path.join(BASE_PATH, '**','*.mp4')\n",
    "\n",
    "#this specifies the sequence length will process by LSTM\n",
    "SEQUENCE_LENGTH = 40\n",
    "BATCH_SIZE = 16\n",
    "print(VIDEOS_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 4:  sample the video --do not process every frame\n",
    "PART 1: define function frame_generator() that creates Sequence_length samples by taking every Kth sample were K= num_frames_in_video / SEQUENCE LENGTH     PART 2: you load the DataSet and specify the output will be frames of size 299x299 x3(rgb) AND you create batches of 16 together at a time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def frame_generator():\n",
    "    video_paths = tf.io.gfile.glob(VIDEOS_PATH)\n",
    "    np.random.shuffle(video_paths)\n",
    "    for video_path in video_paths:\n",
    "        frames = []\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        num_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        sample_every_frame = max(1, num_frames // SEQUENCE_LENGTH)\n",
    "        current_frame = 0\n",
    "        \n",
    "        label = os.path.basename(os.path.dirname(video_path))\n",
    "        \n",
    "        max_images = SEQUENCE_LENGTH\n",
    "        while True:\n",
    "            success, frame = cap.read()\n",
    "            if not success:\n",
    "                break\n",
    "                \n",
    "            if current_frame % sample_every_frame == 0:\n",
    "                frame = frame[:, :, ::-1]\n",
    "                img = tf.image.resize(frame, (224, 224))\n",
    "                img = tf.keras.applications.mobilenet_v2.preprocess_input(img)\n",
    "                max_images -= 1\n",
    "                yield img, video_path\n",
    "                \n",
    "            if max_images == 0:\n",
    "                break\n",
    "            current_frame += 1\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.from_generator(frame_generator,\n",
    "                                         output_types=(tf.float32, tf.string),\n",
    "                                         output_shapes=((224, 224, 3), ()))\n",
    "\n",
    "dataset = dataset.batch(BATCH_SIZE).prefetch(tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 5: \n",
    "For Feature Extraction we are going to use a existing CNN model called Inception V3 which is built into TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mobilenet_v2 = tf.keras.applications.mobilenet_v2.MobileNetV2(input_shape=(224,224,3), include_top=False, weights='imagenet')\n",
    "x = mobilenet_v2.output\n",
    "\n",
    "# We add Average Pooling to transform the feature map from\n",
    "# 8 * 8 * 2048 to 1 x 2048, as we don't need spatial information\n",
    "pooling_output = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
    "feature_extraction_model = tf.keras.Model(mobilenet_v2.input,pooling_output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 6: \n",
    "Extract Features using our InceptionV3 CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Don't run this for if done with feature extraction \n",
    "current_path = None\n",
    "all_features = []\n",
    "\n",
    "#cycle through the dataset and visit each image, note the tdqm is a progress bar\n",
    "#that updates each time a new iteration is called \n",
    "#call feature_extraction_model above (Inception v3) for the image to extract the features\n",
    "for img, batch_paths in tqdm.tqdm(dataset):\n",
    "    batch_features = feature_extraction_model(img)\n",
    "    #reshape the tensor \n",
    "    batch_features = tf.reshape(batch_features, \n",
    "                              (batch_features.shape[0], -1))\n",
    "    \n",
    "    for features, path in zip(batch_features.numpy(), batch_paths.numpy()):\n",
    "        if path != current_path and current_path is not None:\n",
    "            output_path = current_path.decode().replace('.mp4', '.npy')\n",
    "            np.save(output_path, all_features)\n",
    "            all_features = []\n",
    "            \n",
    "        current_path = path\n",
    "        all_features.append(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 7: \n",
    "Create a MyLabelBinarizer for 2 Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run this\n",
    "\n",
    "class MyLabelBinarizer(LabelBinarizer):\n",
    "    def transform(self, y):\n",
    "        Y = super().transform(y)\n",
    "        if self.y_type_ == 'binary':\n",
    "            return np.hstack((Y, 1-Y))\n",
    "        else:\n",
    "            return Y\n",
    "    def inverse_transform(self, Y, threshold=None):\n",
    "        if self.y_type_ == 'binary':\n",
    "            return super().inverse_transform(Y[:, 0], threshold)\n",
    "        else:\n",
    "            return super().inverse_transform(Y, threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run this\n",
    "LABELS = ['Falling','Walking']\n",
    "encoder = MyLabelBinarizer()\n",
    "encoder.fit(LABELS)\n",
    "print(encoder.classes_)\n",
    "print(encoder.transform(['Falling', 'Walking']))\n",
    "\n",
    "t= encoder.transform(['Falling', 'Walking', 'Walking'])\n",
    "print(t)\n",
    "print(encoder.inverse_transform(t))\n",
    "print(\"length of labrels \" + str(len(LABELS)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 8: \n",
    "    Create the LSTM model:    1) Masking layer  2) LSTM layer with 512 cells, dropout 0.5, recurrent_dropout of 0.5  \n",
    " 3) a fully connected relu activation layer with 256 outputs,  4) a droupout layer 0.5  5) a final decision fully connected layer of putput length of labels  (which is the number of classes) with softmax activation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setup a keras Sequential model with 1) Masking layer  2) LSTM layer with 512 cells, dropout 0.5, recurrent_dropout of 0.5  \n",
    "# 3) a fully connected relu activation layer with 256 outputs,  4) a droupout layer 5) a final decision fully connected layer of length of labels\n",
    "# (which is the number of classes) with softmax activation.\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Masking(mask_value=0.),\n",
    "    tf.keras.layers.LSTM(512, dropout=0.5, recurrent_dropout=0.5),\n",
    "    tf.keras.layers.Dense(256, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.Dense(2, activation='softmax')\n",
    "    #tf.keras.layers.Dense(len(LABELS), activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 8: \n",
    "Setup for the model the Loss function, the Optimizer function, and any metrics want to compute in training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy', 'top_k_categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 9: \n",
    "Setup  the training and test list which are lists of the training filenames.   Note you will need to change the location of these files to point to your location.  Define a function make_generator that returns a generator which will randomly shuffle a file list (either training or testing that will be passed later) and then changes the file extension of the avi files listed in the list to .npy which is our features for that avi video which were calcluated in step 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_file = '/Users/subhi/Downloads/CV-proj3/Data/trainlist.txt'\n",
    "#test_file = '/Users/subhi/Downloads/CV-proj3/Data/testlist.txt'\n",
    "test_file = 'C:/Users/STSC.LNVO-126908.000/Desktop/FallDetection/trainlist.txt'\n",
    "train_file = 'C:/Users/STSC.LNVO-126908.000/Desktop/FallDetection/testlist.txt'\n",
    "\n",
    "with open(test_file) as f:\n",
    "    test_list = [row.strip() for row in list(f)]\n",
    "\n",
    "with open(train_file) as f:\n",
    "     train_list = [row.strip() for row in list(f)]\n",
    "    #train_list = [row.split(' ')[0] for row in train_list]\n",
    "#print(train_list)\n",
    "def make_generator(file_list):\n",
    "    def generator():\n",
    "        np.random.shuffle(file_list)\n",
    "        for path in file_list:\n",
    "            full_path = os.path.join(BASE_PATH, path).replace('.mp4', '.npy')\n",
    "            \n",
    "            label = os.path.basename(os.path.dirname(path))\n",
    "            features = np.load(full_path)\n",
    "            \n",
    "            \n",
    "            padded_sequence = np.zeros((SEQUENCE_LENGTH, 1280))\n",
    "            padded_sequence[0:len(features)] = np.array(features)\n",
    "            \n",
    "            transformed_label = encoder.transform([label])\n",
    "            \n",
    "            yield padded_sequence, transformed_label[0]\n",
    "    return generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(test_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 10: \n",
    "Setup the train_dataset and valid_dataset (validation/testing).   Here we setting up training batch sets of 16.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for tesnorflow 2.*\n",
    "train_dataset = tf.data.Dataset.from_generator(make_generator(train_list),\n",
    "                output_types=(tf.float32, tf.int16),\n",
    "                output_shapes=((SEQUENCE_LENGTH, 1280), (len(LABELS))))\n",
    "                 \n",
    "\n",
    "train_dataset = train_dataset.batch(16,drop_remainder=True).prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "\n",
    "valid_dataset = tf.data.Dataset.from_generator(make_generator(test_list),\n",
    "                 output_types=(tf.float32, tf.int16),\n",
    "                 output_shapes=((SEQUENCE_LENGTH, 1280), (len(LABELS))))\n",
    "valid_dataset = valid_dataset.batch(16,drop_remainder=True).prefetch(tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for tensorflow 1.1.4\n",
    "train_dataset = tf.data.Dataset.from_generator(make_generator(train_list),\n",
    "                 output_types=(tf.float32, tf.int16),\n",
    "                 output_shapes=(tf.TensorShape([SEQUENCE_LENGTH, 1280]), tf.TensorShape([len(LABELS)])))\n",
    "\n",
    "train_dataset = train_dataset.batch(16,drop_remainder=True).prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "\n",
    "valid_dataset = tf.data.Dataset.from_generator(make_generator(test_list),\n",
    "                 output_types=(tf.float32, tf.int16),\n",
    "                 output_shapes=(tf.TensorShape([SEQUENCE_LENGTH, 1280]), tf.TensorShape([len(LABELS)])))\n",
    "valid_dataset = valid_dataset.batch(16,drop_remainder=True).prefetch(tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(valid_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DATA_PATH = 'C:/Users/STSC.LNVO-126908.000/Desktop/FallDetection'\n",
    "mylog_dir = os.path.join( BASE_DATA_PATH, \"train_log\")\n",
    "print(\"Mylog directory = \" + mylog_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tf 2.0\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(os.path.join('tmp'), update_freq=1000)\n",
    "#model.fit(train_dataset, epochs=1, callbacks=[tensorboard_callback], validation_data=valid_dataset)\n",
    "model.fit(train_dataset, epochs=1,validation_data=valid_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tf 1.1.4\n",
    "model.fit(train_dataset,epochs=1, validation_data= valid_dataset, validation_steps=4,verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=mylog_dir, update_freq=1000)\n",
    "print(os.path.join(mylog_dir, 'train'))\n",
    "#following call works for tensorflow 1.1.4\n",
    "model.fit(train_dataset, \n",
    "          epochs=17, \n",
    "          validation_data=valid_dataset, \n",
    "          callbacks=[tensorboard_callback])\n",
    "\n",
    "#model.fit(train_dataset, epochs=17, callbacks=[tensorboard_callback], validation_data=valid_dataset, validation_steps=4, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.file=os.path.join(BASE_PATH,'my_model.h5')\n",
    "model.save(model.file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_model()\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "log_dir=\"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "model.fit(x=x_train, \n",
    "          y=y_train, \n",
    "          epochs=5, \n",
    "          validation_data=(x_test, y_test), \n",
    "          callbacks=[tensorboard_callback])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 12:  save the tensorflow model to an h5 file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DATA_PATH = '/Users/subhi/Downloads/CV-Proj3/Data'\n",
    "model_file = os.path.join(BASE_DATA_PATH, 'my_model.h5')\n",
    "# Save the entire model to a HDF5 file.\n",
    "# The '.h5' extension indicates that the model shuold be saved to HDF5.\n",
    "model.save(model_file) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 13: try to conver the model to tflite --- Support to come 2019 (when?)--Curently LSTM conversion to TFLite NOT supported"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from tensorflow import lite\n",
    "tflite_file  = os.path.join(BASE_DATA_PATH, 'my_tflite_model.tflite')\n",
    "print(\" want to save tflite_file\" + tflite_file)\n",
    "# Convert the model.\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#now save the tflite model to the file\n",
    "#tflite_model.save(tflite_file)   #Note this does not seem to work although in google documentation\n",
    "open(tflite_file, \"wb\").write(tflite_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 14: run evaluation on the test data feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the test data using model\n",
    "\n",
    "\n",
    "# Evaluate the model on the test data using `evaluate`\n",
    "print('\\n# Evaluate on test data')\n",
    "\n",
    "# NOTE: should have separate test data but, only have validation data\n",
    "#results = model.evaluate_generator(val_data_gen, verbose=1)\n",
    "results = model.evaluate(valid_dataset, verbose=1)\n",
    "print('test loss, test acc:', results)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 15: Run predictions on the test data feature extracted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make predictions\n",
    "\n",
    "# Generate predictions (probabilities -- the output of the last layer)\n",
    "# on new data using `predict`\n",
    "print('\\n# Generate predictions ')\n",
    "predictions = model.predict(valid_dataset, verbose=1 )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print out prediction info for validation data set (as do not have separate test data set)\n",
    "print('predictions shape:', predictions.shape)\n",
    "print(predictions)\n",
    "print(len(predictions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
